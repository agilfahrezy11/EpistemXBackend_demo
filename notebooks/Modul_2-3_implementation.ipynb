{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respons sistem 0.0\n",
    "import ee\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- User Input -----\n",
    "# ----- Module 1 -----\n",
    "# 1. AOI file path\n",
    "UploadAOIVect = False\n",
    "AOIVectPath = 'C:/Users/fmahezs/epistem/modules_notebook/data/aoi_pedam.shp'\n",
    "# AOIEePath = 'projects/ee-fazamahezs/assets/KHG_sample2'\n",
    "AOIEePath = 'projects/ee-fazamahezs/assets/aoi_pedam'\n",
    "\n",
    "# Define AOI\n",
    "if UploadAOIVect:\n",
    "    gdf = gpd.read_file(AOIVectPath)\n",
    "\n",
    "    # Convert to EE FeatureCollection\n",
    "    features = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        geom = ee.Geometry(mapping(row.geometry))\n",
    "        features.append(ee.Feature(geom, {}))\n",
    "    AOI = ee.FeatureCollection(features)#.geometry()\n",
    "else:\n",
    "    AOI = ee.FeatureCollection(AOIEePath)#.geometry()\n",
    "    \n",
    "# 2. Landsat image\n",
    "# ClippedImage = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199dcf8",
   "metadata": {},
   "source": [
    "# Modul 2 (Define Classification Scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24439a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module and functions\n",
    "import pandas as pd\n",
    "from epistemx.module_2 import LULCSchemeClass, SaveClassificationScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ff06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data Input -----\n",
    "# 1. Choose default LULC scheme\n",
    "ReferenceDataSource = False\n",
    "\n",
    "# 1. Option to manual input or upload csv\n",
    "ManualScheme = False # set as 'false' if you want to upload your own classes in csv file\n",
    "\n",
    "# 2. Upload csv file if ManualScheme is 'false'\n",
    "# LULCTablePath = \"lc_tbl_KHG.csv\"\n",
    "LULCTablePath = \"C:/Users/fmahezs/epistem/modules_notebook/data/lc_pedamaran.csv\" # it can accept both string path and earth engine asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System respons 2.1 -----\n",
    "if ReferenceDataSource == True:\n",
    "    # ----- System respons 2.1.c -----\n",
    "    # LULCTable = ee.FeatureCollection() # call the restore+ classification scheme\n",
    "    pass \n",
    "else:\n",
    "    if ManualScheme == True:\n",
    "        # ----- System respons 2.1.b (Manual/Interactive) -----\n",
    "        LULCClassifier = LULCSchemeClass()\n",
    "        LULCClassifier.Display()\n",
    "    else:\n",
    "        # ----- System respons 2.1.a (Load from Path/Asset) -----\n",
    "        try:\n",
    "            # Check input data source\n",
    "            if isinstance(LULCTablePath, str) and (LULCTablePath.startswith('users/') or LULCTablePath.startswith('projects/') or LULCTablePath.startswith('ft:')):\n",
    "                LULCTable = ee.FeatureCollection(LULCTablePath)\n",
    "                print(f\"Loaded Earth Engine FeatureCollection from Asset ID: {LULCTablePath}\")\n",
    "            elif isinstance(LULCTablePath, str):\n",
    "                LULCTable = pd.read_csv(LULCTablePath)\n",
    "                print(f\"Loaded {len(LULCTable)} land cover/use classes from local CSV file: {LULCTablePath}\")\n",
    "            else:\n",
    "                raise TypeError(\"LULCTablePath must be a string file path or an Earth Engine Asset ID.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LULC Table from path/asset {LULCTablePath}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System respons 2.2 -----\n",
    "if ManualScheme == True:\n",
    "    # Define variable\n",
    "    LULCTable = LULCClassifier.GetDataframe()\n",
    "    print(\"\\nDefined Land cover/use classes:\")\n",
    "    display(LULCTable)\n",
    "else:\n",
    "    print(\"\\nDefined Land cover/use classes:\")\n",
    "    display(LULCTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9df09",
   "metadata": {},
   "source": [
    "# Modul 3 (Define LULC Data Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modul 3a \n",
    "# Import modules and functions\n",
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import ipysheet as sheet\n",
    "from shapely.geometry import mapping\n",
    "from epistemx.module_3a import InputCheck, SyncTrainData, SplitTrainData, LULCSamplingTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eafb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data Input -----\n",
    "# 1. Decision to upload data\n",
    "UploadTrainData = False\n",
    "\n",
    "# 2. Training data file path (if UploadTrainData is true)\n",
    "TrainVectPath  = 'C:/Users/fmahezs/epistem/modules_notebook/data/Pedamaran_sample.shp'\n",
    "TrainEePath = 'projects/ee-fazamahezs/assets/pedamaran_points'\n",
    "#TrainEePath = 'projects/ee-fazamahezs/assets/Pedamaran_sample'\n",
    "\n",
    "# 3. Split data training\n",
    "SplitTrain = True\n",
    "TrainSplitPct = 0.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.1 -----\n",
    "# Check if user chose default classification scheme in module 2\n",
    "# if ReferenceDataSource == True:\n",
    "#     InputCheck.ValidateVariable('AOI')\n",
    "#     LULCTable = ee.FeatureCollection() # define restore+ LULC scheme\n",
    "#     TrainDataRaw = ee.FeatureCollection('projects/ee-rg2icraf/assets/Indonesia_lulc_Sample')\n",
    "# else:\n",
    "#     InputCheck.ValidateVariable('LULCTable', 'AOI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0223bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.2 -----\n",
    "if UploadTrainData == True:\n",
    "\n",
    "    # --- System response 3.2.a ---\n",
    "    # Load training data\n",
    "    TrainDataDict = SyncTrainData.LoadTrainData(\n",
    "        landcover_df=LULCTable,\n",
    "        aoi_geometry=AOI,\n",
    "        training_shp_path=TrainVectPath if UploadTrainData else None,\n",
    "        training_ee_path=None if UploadTrainData else TrainEePath\n",
    "    )\n",
    "\n",
    "    # Set class field\n",
    "    TrainDataDict = SyncTrainData.SetClassField(TrainDataDict, 'LULC_Type')\n",
    "\n",
    "    # Validate classes\n",
    "    TrainDataDict = SyncTrainData.ValidClass(TrainDataDict)\n",
    "\n",
    "    # Check sample sufficiency\n",
    "    TrainDataDict = SyncTrainData.CheckSufficiency(TrainDataDict, min_samples=20)\n",
    "\n",
    "    # Filter by AOI\n",
    "    TrainDataDict = SyncTrainData.FilterTrainAoi(TrainDataDict)\n",
    "\n",
    "    # Create training data table\n",
    "    table_df, total_samples, insufficient_df = SyncTrainData.TrainDataRaw(\n",
    "        training_data=TrainDataDict.get('training_data'),\n",
    "        landcover_df=TrainDataDict.get('landcover_df'),\n",
    "        class_field=TrainDataDict.get('class_field')\n",
    "    )\n",
    "\n",
    "    # --- Print summary ---\n",
    "    vr = TrainDataDict.get('validation_results', {})\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING DATA SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total training points loaded     : {vr.get('total_points', 'N/A')}\")\n",
    "    print(f\"Points after class filtering     : {vr.get('points_after_class_filter', 'N/A')}\")\n",
    "    print(f\"Valid points (inside AOI)        : {vr.get('valid_points', 'N/A')}\")\n",
    "    print(f\"Invalid classes found            : {len(vr.get('invalid_classes', []))}\")\n",
    "    print(f\"Points outside AOI               : {len(vr.get('outside_aoi', []))}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # --- Display the main table ---\n",
    "    if table_df is not None and not table_df.empty:\n",
    "        display_df = table_df.copy()\n",
    "        if 'Percentage' in display_df.columns:\n",
    "            display_df['Percentage'] = display_df['Percentage'].apply(\n",
    "                lambda x: f\"{x:.2f}%\" if isinstance(x, (int, float)) else x\n",
    "            )\n",
    "        display(display_df)\n",
    "    else:\n",
    "        print(\"No valid training data available to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951dc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.2.b -----\n",
    "if UploadTrainData == True:    \n",
    "\n",
    "    if insufficient_df is not None:\n",
    "        # Preview the insufficient classes\n",
    "        print(f\"\\n{len(insufficient_df)} classes with insufficient samples:\")\n",
    "        insufficient_display = insufficient_df.copy()\n",
    "        insufficient_display['Percentage'] = insufficient_display['Percentage'].apply(lambda x: f\"{x:.2f}%\")\n",
    "        display(insufficient_display)\n",
    "\n",
    "        # Add insufficient sample\n",
    "\n",
    "        # Define table\n",
    "        TrainDataRecap = TrainDataDict['training_data']\n",
    "    else:\n",
    "        print(f\"\\nAll classes have sufficient samples!\")\n",
    "\n",
    "        # Define table\n",
    "        TrainDataRecap = TrainDataDict['training_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eab256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.3 -----\n",
    "if UploadTrainData == False:\n",
    "\n",
    "    # ----- System response 3.3.a -----\n",
    "    tool = LULCSamplingTool(\n",
    "    lulc_dataframe=LULCTable,\n",
    "    aoi_ee_featurecollection=AOI\n",
    "    )\n",
    "\n",
    "    tool.Display()\n",
    "\n",
    "    # Export training data\n",
    "    tool.SaveTrainingData(\"results/training_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UploadTrainData == False:\n",
    "    # ----- System response 3.3.a -----\n",
    "    # Convert sampling tool summary to individual point records\n",
    "    training_points = []\n",
    "    \n",
    "    for _, row in tool.TrainDataSampling.iterrows():\n",
    "        if row['Points'] > 0 and row['Coordinates']:\n",
    "            # Parse coordinates string\n",
    "            coord_pairs = row['Coordinates'].split('; ')\n",
    "            for coord_pair in coord_pairs:\n",
    "                if coord_pair.strip():\n",
    "                    # Remove parentheses and split\n",
    "                    coord_pair = coord_pair.strip('()')\n",
    "                    try:\n",
    "                        lat, lon = map(float, coord_pair.split(', '))\n",
    "                        training_points.append({\n",
    "                            'kelas': row['ID'],\n",
    "                            'LULC_Type': row['LULC_Type'],\n",
    "                            'latitude': lat,\n",
    "                            'longitude': lon\n",
    "                        })\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    if training_points:\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Point\n",
    "        \n",
    "        geometries = [Point(point['longitude'], point['latitude']) for point in training_points]\n",
    "        TrainDataRecap = gpd.GeoDataFrame(training_points, geometry=geometries, crs='EPSG:4326')\n",
    "    else:\n",
    "        print(\"No training points collected from sampling tool!\")\n",
    "        TrainDataRecap = pd.DataFrame(columns=['kelas', 'LULC_Type', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.4 -----\n",
    "print(f\"Recapitulation of selected training data sample:\")\n",
    "RecapTable = (TrainDataRecap.groupby(['kelas', 'LULC_Type'])\n",
    "                  .size()\n",
    "                  .reset_index(name='Samples')\n",
    "                  .rename(columns={'kelas': 'ID'})\n",
    "                  .sort_values('ID')\n",
    "                  .reset_index(drop=True))\n",
    "\n",
    "display(RecapTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95713c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- System response 3.5 -----\n",
    "# Split data training\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split data process\n",
    "if SplitTrain == True:\n",
    "\n",
    "    # Split training data\n",
    "    TrainDataFinal, ValidDataFinal = SplitTrainData.SplitProcess(\n",
    "        TrainDataRecap, \n",
    "        TrainSplitPct=TrainSplitPct, \n",
    "        random_state=123\n",
    "    )\n",
    "\n",
    "    # Print split statistics\n",
    "    print(f\"Split data summary\")\n",
    "    print(f\" - Training data: {len(TrainDataFinal)} samples\")\n",
    "    print(f\" - Validation data: {len(ValidDataFinal)} samples\")\n",
    "    print(f\" - Total: {len(TrainDataRecap)} samples\\n\")\n",
    "\n",
    "    # Display preview\n",
    "    print(f\"Training data head:\")\n",
    "    display(TrainDataFinal.head())\n",
    "    print(f\"Validation data head:\")\n",
    "    display(ValidDataFinal.head())\n",
    "\n",
    "    # Plot the splitted data\n",
    "    Map = SplitTrainData.PlotTrainValidInteractive(TrainDataFinal, ValidDataFinal, AOI)\n",
    "    Map\n",
    "\n",
    "else:\n",
    "    print(\"Split not performed. Using entire dataset as training.\")\n",
    "    TrainDataFinal = TrainDataRecap\n",
    "    ValidDataFinal = None\n",
    "\n",
    "    # Display preview\n",
    "    print(f\"Training data head:\")\n",
    "    display(TrainDataFinal.head())\n",
    "\n",
    "    # Plot the training data\n",
    "    Map = SplitTrainData.PlotTrainValidInteractive(TrainDataFinal, AOI)\n",
    "    Map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (epistemx)",
   "language": "python",
   "name": "epistemx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
